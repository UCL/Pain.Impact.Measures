---
title: "Measuring impact of pain in aging population using existing items in ELSA"
subtitle: "*w2 WAVE* ELSA HARMONIZED DATA"
output: 
#  html_document:
  github_document:
    toc: True
date: "`r format(Sys.time(), '%d %b %Y')`"
df_print: paged
editor_options: 
  markdown: 
    wrap: 72
    justify: True
---

```{r setup, eval=TRUE,echo=FALSE,warning=FALSE,message=FALSE,error=FALSE}

library("tidyverse")
library("dplyr")
library("reshape2")
library("lavaan")
#library("semPlot")
library("ggplot2")
library("pheatmap")
library("RColorBrewer") # heatmap color palettes customisation
library("survey")

library("naniar")


library("psych") # required for Dimensionality()
library("nFactors")  # required for Dimensionality()
library("boot") # required by internalConsistency()





```

# INTRODUCTION







## AIM

The overal aim of this analysis is to present a measurement model that can capture  
the dimensions included in the NIH screenig tool for chronic pain impact. 


The goas of this analysis are:

0.  to explore the dimensionality of the items selected in the previus stages
1.  to assess construct validity of the measurement models proposed
2.  to achieve one or more sets of items to candidate for further analysis

# METHOD

## SAMPLE

```{r, eval=TRUE,echo=T,warning=FALSE,message=FALSE,error=FALSE}

## this is marked as "all" because includes both respondents and partners
H_elsa_all<-read.csv("c:/Users/d.vitali/Desktop/Github/CRIISP-WP5/data/clean/H_w1_9_ALL.tab")


```

This dataframe includes Harmonized ELSA data ([Banks et al. 2024](#references)) from all waves of ELSA (harmonised g2 dataset). 

For the purpose of this analysis I will focus on Wave 2 data because this wave is the first implementing one of the key variables that are being evaluated here


### Variable naming convention

Each individual is uniquely identified by the unique identifier:

    idauniq

Variable names in the Harmonized ELSA Data follow a consistent pattern
(e.g.):

    r2HLTHLM = (R)espondent, Wave(1), Health problem limiting work(HLTHLM)

1.  The first character indicates whether the variable refers to the
    reference person ("R"), spouse ("S"), the full household ("HH"), and
    a financial unit household ("H").
2.  The second character indicates the wave to which the variable
    pertains: "1", "2", "3", "4", "5", "6" , "7", "8", "9", or "A". The
    "A" indicates "all", i.e. the variable is not specific to any single
    wave.
3.  The remaining characters describe the concept that the variable
    captures

| Code  | Reason for missing           |
|-------|:-----------------------------|
| NA    | did not respond to this wave |
| (-4)  | Don't know                   |
| (-18) | Refused                      |
| (-13) | Other missing                |
| (-16) | Proxy                        |

: **Table 2.** Coding of missing data in the H.Elsa dataset

### Cohorts, Strata and Clusters

In this wave one there are 3 identified cohorts:

1.  original HSE sample (HSE 1998-2001) (see [Mindell et al. 2012](#references)) 
2.  partner of an original sample respondent 
3.  young partner of an original sample respondent
```{r, eval=TRUE,echo=F,warning=FALSE,message=FALSE,error=FALSE}
#table("cohorts"= H_elsa_all$r2cohort_e)
```

| case | desc                                | N    |
|------|:------------------------------------|------|
| 1    | Original HSE sample                 | 11595|
| 2    | Partner of an orig. sample resp.    | 138  |
| 3    | Young partner an orig. sample resp. | 664  |

: **Table 3.** Cohorts and respective count of individuals for the current wave

Therefore selecting only original HSE sample's core respondents and excluding their partners.

```{r, eval=TRUE,echo=F,warning=FALSE,message=FALSE,error=FALSE}
H_elsa_w2_ <-H_elsa_all %>% dplyr::filter((r2cohort_e == 1))

table("cohort" = H_elsa_w2_$r2cohort_e)
```


### Participants in W2
```{r, eval=TRUE,echo=F,warning=FALSE,message=FALSE,error=FALSE}
table("inwave2:" = H_elsa_all$inw2)

H_elsa_w2_ <-H_elsa_all %>% dplyr::filter((inw2 == 1))
```
Participants responding to wave 2 was: `r table("inwave2:" = H_elsa_all$inw2)[2]`

Down `r table("inwave2:" = H_elsa_all$inw1)[2]-table("inwave2:" = H_elsa_all$inw2)[2]` from wave 1.

### Chronic pain indicator

To identify respondents who erienced persistent pain symptoms, all participants were required to report pain severities of  moderate or greater at both the analysed and previous ELSA data collection interval"

```{r}
# load the chronic pain categorising function
source("c:/Users/d.vitali/Desktop/Github/Pain.Impact.Measures/ELSA Models/chronic_pain_var.R")

# Create chrpain variables for waves x to 9
waves <- 1:9
for (wave in waves) {
  H_elsa_w2_ <- create_chrpain_variable(H_elsa_w2_, wave)
}

#table(H_elsa_w2_$r2chrpain)



```


### Selecting 50yo+

Of this sample of the elsa current wave respondents  
I am excluding respondent aged < 50 as ELSA is focused on english population aged 50 and older.

```{r, eval=TRUE,echo=F,warning=FALSE,message=FALSE,error=FALSE}


# removing all participants who younger than 50
H_elsa_w2_ <-H_elsa_w2_ %>% dplyr::filter(is.na(r2agey) |r2agey>49 )
```

## MEASURES
### Participation to general activities

loading the ELSA non harmonised dataset  for wave 1. We need this to get the variable
"helim". This correspond to the question:

    Q: helim. (Does this / Do these) illness(es) or disability(ies) limit your activities in any way?

**Cut-off criterion.** "limit your activity" is the cut-off criterion proposed to respondents

**Anchor in pain.** NO i.e. lacking explicit causal link between the limitation examined and pain.
**Anchor in health.** YES i.e. proposing a explicit causal between the limitation examined and health problem.


```{r, echo=FALSE}
elsa_w2<-read.csv("c:/Users/d.vitali/OneDrive - University College London/CRIISP/00_WP5/00_WP5_data/5050tab/UKDA-5050-tab/tab/wave_2_core_data_v4.tab", sep = "\t")
elsa_w2$Helim %>% table
elsa_w2<-elsa_w2 %>% filter(idauniq %in% H_elsa_w2_$idauniq)
H_elsa_w2_<-merge(elsa_w2[c("idauniq","Helim")],H_elsa_w2_, by = "idauniq", all = T)

table("Helim"=H_elsa_w2_$Helim)
```

.  "helim" = -1 : (not applicable, i.e. the do not report an illness, disability or infirmity)
.  "helim" = 1 : No (does not limit the activities)
.  "helim" = 2 : Yes (limits the activites)

we rename this variable as "hlmact".

```{r, echo=FALSE}
H_elsa_w2_$hlmact<- ifelse(H_elsa_w2_$Helim == 1,
                          1,
                          ifelse(H_elsa_w2_$Helim == 2 | H_elsa_w2_$Helim == -1,
                                 0,
                                 NA)
                          )
H_elsa_w2_$hlmact %>% table
```

### Participation to Work Activities


    Do you have any health problem or disability that limits the kind or amount of paid work you 
    could do, should you want to?

**Cut-off criterion.** "limit" is the cut-off criterion proposed to respondents.

**Anchor in pain.** NO i.e. lacking explicit causal link between the limitation examined and pain.
**Anchor in health.** YES i.e. proposing a explicit causal between the limitation examined and health problem.

These Items are not anchored in pain (i.e. lacking  "because of pain" pain or in "because of health problem or disability"

| VarName   | Description                 								                           |
|:----------|:-----------------------------------------------------------------------|
|rXhlthl    | impairment or health problem limits paid work for the respondent       |


```{r, echo=FALSE}
                        
table(H_elsa_w2_$r2hlthlm)

```

.  "r2hlthlm" = -16 : interview given via proxy (these will be filtered out)
.  "r2hlthlm" = -13 : missing
.  "r2hlthlm" = -4 : Don't know
.  "r2hlthlm" = 1 : NO  (Does not limit work)
.  "r2hlthlm" = 2 : Yes (limits work)


### ADL+IADL items in ELSA

The ADL ([Katz 1963](#references))refers to activities that are fundamental to living in a social world as they 
describe ways in which an individual enables oneself's basic survival and well-being, 
such as bathing, toileting, dressing and eating.

The IADL ([Lawton and Brody 1969](#references)) refers to activities to support daily life at home and outside in the community.
These activity require more complex skills and more complex interactions than those described in the ADLs.


In H.ELSA the ADL and AIDL items and three additional mobility-focused items. 
All items are of type "dichotomous" and they are scored as 1 if the respondent 
indicates a difficulty with a particular activity, or as 0 if they do not indicate 
a difficulty.


    Here are a few more everyday activities. Please tell me if you have any 
    difficulty with these because of a physical, mental, emotional or memory 
    problem. Again exclude any difficulties you ect to last less than 
    three months.
    
**Cut-off criterion.** "difficulty" is the cut-off criterion proposed to respondents.

**Anchor in pain.** NO i.e. lacking explicit causal link between the limitation examined and pain.
**Anchor in health.** YES i.e. proposing a explicit causal between the limitation examined and health problem.


| VarName    | scale | Waves | Description:                                                 |
|:-----------|:------|:-----:|:-------------------------------------------------------------|
| rxdressa   | ADL   |  1-9  | dressing including putting on shoes and socks                |
| rxwalkra   | ADL   |  1-9  | walking across a room                                        |
| rxbatha    | ADL   |  1-9  | bathing or showering                                         |
| rxeata     | ADL   |  1-9  | eating such as cutting up food                               |
| rxbeda     | ADL   |  1-9  | getting in and out of bed                                    |
| rxtoilta   | ADL   |  1-9  | using the toilet including getting up or down                |
| rxmapa     | IADL  |  1-9  | using map to figure out how to get around strange place      |
| rxmealsa   | IADL  |  1-9  | preparing a hot meal                                         |
| rxshopa    | IADL  |  1-9  | shopping for groceries                                       |
| rxphonea   | IADL  |  1-9  | making telephone calls                                       |
| rxmedsa    | IADL  |  1-9  | taking medications                                           |
| rxhousewka | IADL  |  1-9  | doing work around house and garden                           |
| r2chaira   | Mobil |  1-9  | Diff-Get up fr chair                                         |
| r2clim1a   | Mobil |  1-9  | Diff-Clmb 1 flt str                                          |
| r2lifta    | Mobil |  1-9  | Diff-Lift/carry 10lbs                                        |


: **Table 5.** Variable names, description, scale origin, and ELSA waves
in which each item has been implemented.

### Defining list of Variables

List of the proposed variables
```{r}
varlist<-c("idauniq",
           "r2dressa", "r2walkra", "r2batha"   , "r2eata"  , "r2beda"  , "r2toilta",
           "r2shopa" , "r2mealsa", "r2housewka", "r2mapa"  , "r2phonea", "r2moneya", "r2medsa" , 
           "r2hlthlm", "hlmact",
           "r2chaira", "r2clim1a","r2lifta"
           )
```

### w2 Wave: Missing Data in  sample

Missing data for any considered indicator was treated with listwise
deletion and reported in the table below.

```{r, eval=TRUE,echo=F,warning=FALSE,message=FALSE,error=FALSE}
# -26 = Not retired
# -23 = currently working
# -19 = self-employed
# -18 = Refuse
# -16 = Proxy - Not applicable question was skipped because the interview was by proxy
# -14 = not in labour force / not applicable
# -13 = Missing
# -11 = no kids
# -12 = not looking for a new job
# -4  = DK
# -3  = No self-completion interview

# check NAs
#sapply(H_elsa_w2_raw, function(x) sum(is.na(x)))
#H_elsa_w2_raw$r2painfr %>% table## Printing missing values count for each item ( vars < 0)


summry_missing<-data.frame(matrix(ncol = 3, nrow = 0))

for(i in varlist){
  c_<-sum(H_elsa_w2_[i] < 0 | is.na(H_elsa_w2_[i] ))
  t_<-nrow(H_elsa_w2_)
  summry_missing<-rbind(summry_missing, data.frame(i, c_, t_))
}
colnames(summry_missing)<-(c("VarName", "N. Missing", "Total"))
print(summry_missing)

temp<-H_elsa_w2_[varlist[-1]]
temp[temp<0]<-NA
vis_miss(temp,show_perc = F)
```


## ANALYSIS

Combining:

  * ADL
  * IADL items
  * functional ability items
  * r2hlthl (health impairment prevents from doing my work)
  * helim (health inmpairment prevents engaging activities)

1.  PA + EFA to evaluate construct validity and dimensionality
2.  CFA to evaluate the measurement model


### listwise deletion of missing data

Selecting complete cases
```{r, eval=TRUE,warning=FALSE,message=FALSE,error=FALSE}

#Creating a ADL&IADL specific dataframe so that

# -   columns: item name
# -   rows: item response


LV_items<-H_elsa_w2_[,varlist] 


# selecting complete cases listwise 
LV_items<- LV_items %>% filter(if_all(.fns = ~. >= 0))

```

total sample available for analysis `r nrow(LV_items)`.

### Age and gender distribution

```{r}
plotdata<-H_elsa_w2_ %>% filter(idauniq %in% LV_items$idauniq)

plotdata$ragender <-factor(plotdata$ragender, levels = c(1,2), labels = c("Male", "Female"))


# Create a summary data frame with counts
counts <- plotdata %>%
  group_by(ragender) %>%
  summarise(count = n())

# Plot with bar and frequency count labels
ggplot(plotdata, aes(x = ragender)) +
  geom_bar(fill = "blue", color = "black", alpha = .4) +
  geom_text(data = counts, aes(x = ragender, y = count, label = count), 
            vjust = -0.5, color = "black") +
  labs(title = "Distribution of Sex at birth",
       x = "category",
       y = "Count") +
  theme_minimal()

# proportions
(table(plotdata$ragender, useNA = "ifany")%>% prop.table())*100 %>% round(2)

```


```{r echo=F}

plotdata<-H_elsa_w2_ %>% filter(idauniq %in% LV_items$idauniq)

# Gender
# 1 = Male
# 2 = female
#sum(H_elsa_w2_$r2agey <50) 
plotdata0 <- plotdata[,c("r2agey","ragender")]

colnames(plotdata0)<-c("age","gender")
#table(plotdata0$gender)
plotdata0$gender[plotdata0$gender == 1] <-"M"
plotdata0$gender[plotdata0$gender == 2] <-"W"

plotdata0$gender<-as.factor(plotdata0$gender)

plotdata0$age_group<- cut(plotdata0$age, c(0,29, 39, 49, 59, 69, 79, 89, Inf), c("0-29", "30-39","40-49" ,"50-59", "60-69","70-79","80-89","90+"), include.lowest=TRUE)
plotdata0$age_group<-as.factor(plotdata0$age_group)


ggplot(plotdata0,aes(x=age_group, fill=gender))+
  geom_bar(position="dodge")+
  scale_fill_discrete(name="Gender",
                      breaks=c("M", "W"),
                      labels=c("Male", "Female"))+
  geom_text(stat='count', aes(label=after_stat(count)), position = position_dodge(width = 1), vjust=-.5)+
  ggtitle(paste0("w2 wave: Age Groups by Gender (N = ",nrow(plotdata0),")"))+
  xlab('Age group')+
  ylab('No. of People')

#table(plotdata0$age_group,plotdata0$gender) %>% addmargins()
proptable0<-prop.table(table(plotdata0$age_group,plotdata0$gender))*100
proptable0  %>% addmargins(margin = 1) %>% round(2)


tab0<-table(plotdata0$age_group,plotdata0$gender)
tr_tab0<-rbind(tab0[c(4:7),1],tab0[c(4:7),2])

props<-tr_tab0 %>% addmargins(1)
props[2,c(1:4)]/props[3,c(1:4)]
#chisq.test(tr_tab0, correct = F) # no continuity correction as the table is not 2by2
prop.test(x=tr_tab0[2,],n=addmargins(tr_tab0,1)[3,], correct = F)




```

The age distribution in this sample of subject reporting pain show a majority of 
women in all age groups with the 80-89 range presenting the
highest prevalence of females:

|     | 50-59           | 60-69           | 70-79           | 80-89           | **Tot**         |
|----:|:---------------:|:---------------:|:---------------:|:---------------:|:---------------:|
|M    |`r tab0[4,1]`    |`r tab0[5,1]`    |`r tab0[6,1]`    |`r tab0[7,1]`    |`r sum(tab0[,1])`|
|F    |`r tab0[4,2]`    |`r tab0[5,2]`    |`r tab0[6,2]`    |`r tab0[7,2]`    |`r sum(tab0[,2])`|
|Tot  |`r sum(tab0[4,])`|`r sum(tab0[5,])`|`r sum(tab0[6,])`|`r sum(tab0[7,])`|`r sum(tab0[,1:2])`|


: **Table 4.** Count and proportion of individuals by age and gender
(current wave)

### Ethnicity

```{r}

plotdata$raracem <-factor(plotdata$raracem, levels = c(-13,1,4), labels = c("NA","White", "Non-white"))

table(plotdata$raracem, useNA = "ifany")

# Create a summary data frame with counts
counts <- plotdata %>%
  group_by(raracem) %>%
  summarise(count = n())

# Plot with bar and frequency count labels
ggplot(plotdata, aes(x = raracem)) +
  geom_bar(fill = "blue", color = "black", alpha = .4) +
  geom_text(data = counts, aes(x = raracem, y = count, label = count), 
            vjust = -0.5, color = "black") +
  labs(title = "Distribution of race by white vs non-white category",
       x = "category",
       y = "Count") +
  theme_minimal()

```


### Net total Wealth
```{r echo=FALSE}
elsa_w2_fin<-read.csv("c:/Users/d.vitali/OneDrive - University College London/CRIISP/00_WP5/00_WP5_data/5050tab/UKDA-5050-tab/tab/wave_2_financial_derived_variables.tab", sep = "\t")

H_elsa_w2_<-merge(elsa_w2_fin[c("idauniq","totwq5_bu_s","nettotw_bu_s")],H_elsa_w2_, by = "idauniq", all = T)

plotdata<-H_elsa_w2_ %>% filter(idauniq %in% LV_items$idauniq)
```
Net total wealth (benefit unit level). The sum of savings, investments, physical wealth and housing wealth after financial debt and mortgage debt has been subtracted.


The distrubution of wealth is severely skewed

```{r echo=FALSE}
ggplot(plotdata[which(!is.na(plotdata$nettotw_bu_s)),], aes(x = nettotw_bu_s)) +
  geom_histogram(fill = "blue", color = "black", alpha = .4) +
  scale_x_continuous(labels = scales::comma) +
  labs(title = "Distribution of Wealth",
       x = "Total Net Wealth",
       y = "Count") +
  theme_minimal()

```

and a log transformation of the wealth magnitude axis allows for better visual examination of the distribution

```{r echo=F}
ggplot(plotdata[which(!is.na(plotdata$nettotw_bu_s)),], aes(x = nettotw_bu_s)) +
  geom_histogram(fill = "blue", color = "black", alpha = .4) +
  scale_x_continuous(trans='log10',labels = scales::comma) +
  labs(title = "Distribution of Wealth (log scale)",
       x = "Total Net Wealth ( log scale)",
       y = "Count") +
  theme_minimal()

```


the representation by quintile allows for a more general perspective on this same information

```{r echo=F}
ggplot(plotdata[which(!is.na(plotdata$totwq5_bu_s)),], aes(x = as.factor(totwq5_bu_s), y = nettotw_bu_s)) +
  geom_boxplot() +
  scale_y_continuous(trans='log10',labels = scales::comma) +
  labs(title = "Distribution of Wealth by Quintile",
       x = "Quintile (poorer->wealthier)",
       y = "Wealth (log scale) ")+
  theme_minimal()

table(plotdata$totwq5_bu_s, useNA = "ifany") %>% addmargins()

```


### Construct dimensionality

Construct dimensionality will be assess via Parallel Analysis (PA) and
EFA. The factor structure emerging from these explorative analysis will
then be submitted for CFA for further empirical validation of the
solutions proposed.

The sample available to these analysis is n = `r nrow(H_elsa_w2_)`
therefore it was further partitioned in two randomly selected datasets :

1.  \~ 1/2 from of total sample to used for PA and EFA (LV_items_ex)
2.  the remaining data (LV_items_conf) to be used for CFA 

#### Sample split
Split sample for confirmatory factor analysis. 50% for EFA and then 50 for CFA.
```{r, eval=TRUE,echo=FALSE,warning=FALSE,message=FALSE,error=FALSE}

#creates an array of randomly distributed true of false. the split is 50/50 in this case
set.seed(1234)
ind <- sample(c(TRUE, FALSE), nrow(LV_items), replace=TRUE, prob=c(0.5, 0.5))

# select rows that have "TRUE"
LV_items_ex<-LV_items[ind,]


# select rows that are not "TRUE"
LV_items_conf<-LV_items[!ind,]

# select rows that are not "TRUE"
LV_items_conf<-LV_items_conf[,varlist] # reordered by the Hypothesis that 7:10 are cognitive items
LV_items_ex<-LV_items_ex[,varlist] # reordered by the Hypothesis that 7:10 are cognitive items
```

### Parallel analysis

Parallel analysis will be used to compare the scree of factors of the
observed data with that of a random data matrix (re-sampled from the
original) of the same size. It should be noted that as the sample size
increases to infinite, the approximation of the eigenvalue of the random
data tends to 1.

### Correlation matrix

[Spector and Fleishman (1998)](#references) have pointed out that using pearson
correlation coefficients have been shown to suffer greatly in the
presence of skew data ([Muthen 1988](#references)). This is not uncommon in the
literature and one example of PCA using a person correlation matrix is
found in [Kempen et al (1995)](#references). In the current analysis, I used
tetrachoric correlations ([Spector & Fleishman, 1998](#references)). Tetrachoric
correlations assume that observed dichotomous variables represent
unobserved (latent) continuous variables with a bivariate normal
distribution; the tetrachoric correlation estimates the relationship
between these latent continuous variables.

### EFA

#### Sample

n = `r nrow(LV_items_ex)` (\~1/2) randomly selected from the N= `r nrow(LV_items)` first 
wave respondents studied in this paper. The remaining 1/2 of the N= `r nrow(LV_items)` will 
be used for CFA.

#### Estimator

With regards to the estimator, some (although dated) suggestions point to generalized least 
squares estimation procedure [Spector & Fleishman, 1998; Muthen, 1978, 1988](#references). 
The choice for the GLS estimator comes from [Fleishman (1998)](#references) and 
[Muthen (1978, 1988)](#references).  Arguably ML estimation is a more 
appropriate and accurate method for factor analysis with binary data than other 
methods, such as PCA or PAF, because it makes fewer assumptions about the 
distribution of the data. More recent methodological papers suggest WLSM 
(mean adj weighted least square - see e.g. mplus) which do not assume a 
particular distribution.

#### Tetra- vs Polychoric correlations
Tetrachoric correlation assumes that the binary data have a continuous underlying 
distribution, and estimates the correlation between the underlying continuous 
variables. Tetrachoric correlation is appropriate when the binary variables are 
thought to be generated from an underlying normal distribution. The distribution 
of the binary data may not be accurately approximated by a continuous distribution,
and this can lead to biased estimates of the tetrachoric correlation matrix. 
This bias can be particularly problematic when the binary data are highly skewed 
and therefore should be avoided in this case

Polychoric correlation, on the other hand, is appropriate when the binary variables 
are thought to be generated from an underlying ordinal variable, with multiple 
categories. It assumes that the binary variables have an underlying, unobserved 
continuous variable that generates the observed categories, and estimates the 
correlation between the underlying continuous variables. Polychoric correlation 
is more robust to departures from normality and can provide more accurate 
estimates of the correlation matrix when the binary variables have few categories 
and/or are highly skewed.


    

#### Rotations

Rotations considered:

-   promax
-   varimax

The two factors are expected to be correlated. The correlation
assumption is explored via the application of two rotation methods:
oblique and orthogonal.

1.  Correlated factors assumption (oblique rotation)
    -   Promax rotation is a popular approach with large datasets -
        could overestimate the correlation between factors
    -   Oblimin rotation is discouraged with large datasets but can help
        produce simpler factor structures.
2.  Orthogonal factors assumption

Promax and oblimin rotations are expected to yield a clearer factor
solution here than Varimax which assumes orthogonality between the latent
factors.

[Thurstone's (1947)](#references) five criteria for the "simple" solution were used to
select the rotation method

1.  Each variable should produce at least one zero loading
    (-.1\<$\lambda$\<.1) on some factor.
2.  Each factor should have at least as many zero loadings as there are
    factors.
3.  Each pair of factors should have variables with significant loadings
    on one ($\lambda$ \> .3) and zero loadings on the other.
4.  Each pair of factors should have a large proportion of zero loadings
    on both factors (if there are say four or more factors total).
5.  Each pair of factors should have only a few complex variables (
    $\lambda$ \> .3).

We can think of the goal of rotation and of choosing a particular type
of rotation as seeking something called simple structure. That is, the
rotation is adequate if the result achieves a simple structure.

    

### CFA

#### Sample

The remaining 1/2 of the N= `r nrow(LV_items)`, randomly selected.

#### Estimator

WLSM (mean adj weighted least square - see e.g. mplus) which do not
assume a particular distribution [Newsom 2023](#references). This method imply 
the Diagonalised weighted least square estimator, have robust standard errors and a mean
adjusted $\chi^2$ statistic.

#### Items selection

Round 1: all items with EFA suggested structure Round 2: final item
selection

CFA will inform the removal of items that do not support a single
dimension solution of the proposed scale of disability. Fit of models
will be compared via scaled Chi-Squared Difference Test
($\Delta\chi^2$).

# RESULTS

## Item responses: descriptive statistics


```{r, eval=TRUE,echo=FALSE,warning=FALSE,message=FALSE,error=FALSE}

#str(wp5_df_w2_red)

# exclude missing data

ADL<-c("r2walkra","r2dressa","r2batha","r2eata","r2beda","r2toilta")
IADL<-c("r2shopa", "r2mealsa", "r2housewka","r2mapa", "r2phonea","r2moneya", "r2medsa")
FUNC_additional<-c("r2chaira","r2clim1a","r2lifta")
WORK_additional<-c("r2hlthlm","hlmact")



plotdata1 <- LV_items[ADL] 
plotdata2 <- LV_items[IADL]
plotdata3 <- LV_items[WORK_additional]
plotdata4 <- LV_items[FUNC_additional] 


colnames(plotdata1)<-c(
                    "walking across a room", # a1
                    "dressing",              # a2
                    "bathing or showering",  # a3
                    "eating/cutting up food",# a4
                    "getting in/out of bed", # a5
                    "toilet")                # a6
colnames(plotdata2)<-c(
                    "shopping for groceries",# b4
                    "prep. hot meal",        # b5
                    "work house/garden",
                    "using a map in unknown places", 
                    "telephone calls",       # b1
                    "managing money",        # b2
                    "taking medications"    # b3
                    )

colnames(plotdata3)<-c(
                    "Health limit work",
                    "Health/disability limits actity")
colnames(plotdata4)<-c(
                    "Some Diff-Get up fr chair",
                    "Some Diff-Clmb 1 flt str",
                 
                    "Some Diff-lifting/carrying 10pounds"
                 
                    )
                    
# transforming data from a wide format to a long format
p1 <- melt(plotdata1) #%>% filter(rowSums(plotdata1 <0) ==0))
p2 <- melt(plotdata2) #%>% filter(rowSums(plotdata2 <0) ==0))
p3 <- melt(plotdata3) #%>% filter(rowSums(plotdata3 <0) ==0))
p4 <- melt(plotdata4) #%>% filter(rowSums(plotdata3 <0) ==0))

ggplot(p1,aes(x = value)) + 
    facet_wrap(~variable)+
    labs(title ="ADL w2 l. dataset. Please tell me if you have any difficulty with these..",
         subtitle = "Item response frequencies",
         #caption="Note: 1=..more than once a week; 2= once a week; 3=one three times a month; 4=hardly ever or never"
    )+
    geom_histogram(binwidth = .5)

ggplot(p2,aes(x = value)) + 
    facet_wrap(~variable)+
    labs(title ="IADL w2 l. dataset. Please tell me if you have any difficulty with these..",
         subtitle = "Item response frequencies",
         #caption="Note: 1=..more than once a week; 2= once a week; 3=one three times a month; 4=hardly ever or never"
    )+
    geom_histogram(binwidth = .5)

ggplot(p3,aes(x = value)) + 
    facet_wrap(~variable)+
    labs(title ="Health ... ",
         subtitle = "Item response frequencies",
         #caption="Note: 1=..more than once a week; 2= once a week; 3=one three times a month; 4=hardly ever or never"
    )+
    geom_histogram(binwidth = .5)

ggplot(p4,aes(x = value)) + 
    facet_wrap(~variable)+
    labs(title ="Health imposes ..",
         subtitle = "Item response frequencies",
         #caption="Note: 1=..more than once a week; 2= once a week; 3=one three times a month; 4=hardly ever or never"
    )+
    geom_histogram(binwidth = .5)

summarytab<-cbind(plotdata1,plotdata2,plotdata3,plotdata4)
```



## Ordered proportion of responses

List of item by how often are scored as "1", i.e. experiencing the difficulty.
```{r}
propsummary<-data.frame(Item = colnames(summarytab),
                       Proportion = round(colMeans(summarytab) * 100,2)) 
rownames(propsummary)<-NULL


# Proportion of current wave respondent having difficulties with
# each of the nine IADL ADL Tasks
propsummary[order(propsummary$Proportion, decreasing = TRUE), ] %>% print




```


## GOAL 1: Testing dimensionality of items

### Dimensionality

When ADL and IADL are used as separate scales they seem to present
clearer dimensionality and hierarchical structure but they may also
carry significant DIF bias and intrinsic limitations with regards to
what they measure. As an example of intrinsic limitations, ADL items
cannot discriminate below higher levels of functional disability thereby
making it difficult to use the ADL items alone in populations where the
prevalence of severe levels of disability tends to be low ([Kovar and Lawton 1994](#references)).

On the other hand, when considering ADL+IADL as a single scale, it is
important to observe that the simple aggregation of these indicators in
a deterministic model (e.g. Guttman's model) implies the untenable
assumption of the equivalence of each item's importance ([Thomas et al 1998, Fieo 2011](#references)). 
Some researchers have suggested that rather than using
a deterministic model it is perhaps better to assume a probabilistic
relationship between the item responses and the concept these aim to
represent ([Kempen et al 1995](#references)). Recent work by [Caballero et al. (2017)](#references)
have showed that it is possible to create a compound health metric in
ELSA that can be used to quantify and study the health status of
individuals overtime using a probabilistic model. In addition, and most
relevantly for the larger study in which the present is nested,
[Caballero et al. (2017)](#references) showed that Machine Learning methods can be applied
to identify relationships between socio-demographics and the health
score created.


### Correlation matrix

The following plot shows the (polychoric) correlation matrix in conjunction with the hierarchical
clustering of these variables done by pheatmap() using hclust(). 

```{r, eval=TRUE,echo=T,warning=FALSE,message=FALSE,error=FALSE}


cor0<-polychoric(LV_items_ex[,-1], smooth = F, correct = F)


cor0$rho %>% pheatmap(display_numbers = T, 
                      cluster_rows = T,
                      cluster_cols = T,
                      breaks = seq(0, 1, length.out = 101),
                      main = "Items Polychoric Correlation matrix")
```

The visual examination of the tetrachoric correlation matrix
seems two suggest a two factor solution with "warmer colors" (higher
correlation) with "map,phone,money,med", and a larger group with the rest of the items.


### Hierarchical clustering

The dendrogram visually represents the hierarchical structure of the data, allowing you to interpret the relationships between clusters or data points based on their distances or dissimilarities.

The methods "average" linkage is used as it is less sensitive to the distribution of the variables (these are all dichotomous items)

The vertical axis represents the distances or dissimilarities between clusters or individual data points. Therefore, the height at which two clusters or data points are merged corresponds to the distance or dissimilarity between them. The longer the vertical line, the greater the dissimilarity between the clusters being merged.

```{r}

# treats correlation scores as point in a high dimensional space and calculates the distance between two points
polychor.dist<-as.dist(1 - cor0$rho)

#hclust(polychor.dist, method="complete") %>% plot

# Complete linkage or maximum linkage:

# . Calculates the maximum dissimilarity between pairs of clusters.
# . Tends to produce clusters that are compact and well-separated.
# . Can be sensitive to outliers.

hclust(polychor.dist, method="average") %>% plot

# Average linkage

# . Calculates the average dissimilarity between pairs of clusters.
# . Less sensitive to outliers compared to complete linkage.


```

Items within the closest distance seems to include, in order:

"meal, shop"
"housework,walk across the room"
"getting out of bed", "gettin on/off toilet"
"getting dressed", "using the bath/washing oneselves"
"climbing one flight of stairs", "lifting a weight"
"health limits work", "health limits my activity"



### All items solution 

#### Parallel analysis

This code is performing a parallel analysis to determine the number of
factors that should be retained in a factor analysis of the LV_items_ex
dataset. The number of factors to retain will be determined by comparing
the actual eigenvalues to the mean and standard deviation of the
eigenvalues generated from the bootstrap resampled datasets.

If the aim of parallel analysis is to determine the number of unobserved factors 
that explain the scoring of our items, the difficulty of this process consists with 
the challenge of selecting appropriate communalities. 
Simulation studies showed that different approaches to estimating the communalities 
can under-estimate (or over-) the importance of minor factors. [Revelle 2024](#references)

Here we will use parallel with the option "fa" which estimates the communalities 
based upon a one factor minres solution. Although "minres" is a conservative approach
and will tend underestimate the communalities, [Revelle 2024](#references) is suggested that leads to better solutions on simulated or real data sets.



```{r, eval=TRUE,echo=T,warning=FALSE,message=FALSE,error=FALSE}

pa<-psych::fa.parallel(LV_items_ex[,-1],
                   main = "Parallel analysis round 1",
                   fa ="fa", 
                   fm = "gls", 
                   sim = "F", 
                   error.bars = T, 
                   cor = "poly", 
                   correct = T)

```

Parallel analysis confirms the suggestion of a two factor solution as
well as the expected eigenvalue for the second factor close to 1 as
observed in previous studies.

##### Alternatives to Parallel analysis
Another method to evaluate the number of factors to extract in exploratory factor analysis is to use the "Very simple structure criterion" (VSS). In factor analysis we tend to interpret the results of our factor analysis by focusing on the largest loadings that each variable extress and we tend to consider less important or even ignore the loadings of smaller magnitude. VSS takes this "simplifying" approach and operationalizes it by comparing the original correlation matrix to that reproduced by a simplified version of the original factor matrix. The VSS criterion compares the fit of the simplified model to the original correlation [Revelle & Rocklin 1979](#references).


VSS() is a function in the psych package stands for Very Simple Structure and applies this method. This function takes as input a correlation matrix (in this case we need to specify the sample size). or a dataset (in this case we need to specify which correlations ought to be used). We then specify the extraction method "fm" as "mle" Maximum Likelihood FA. Other factoring methods available are "pa" Principal Axis Factor Analysis,  "minres" stands for minimum residual (OLS) factoring, "pc" Principal Components".
By including the diagonal (setting diagonal = T), the model will focus on the total variance in the dataset (the assumption is that total variance = common variance). In this case we chose to exclude the diagonal to model only shared variance (communality) and ignoring the unique variance. This method aligns with the goal of EFA, which is to identify latent constructs that explain the correlations among observed variables.

```{r, eval=TRUE,echo=T,warning=FALSE,message=FALSE,error=FALSE}

cor_vss<-polychoric(LV_items_ex[,-1], smooth = F, correct = F)


VSS(cor_vss$rho, 
    fm="mle", 
    rotate = "oblimin",
    n=4,
    n.obs = nrow(LV_items_ex[,-1]), 
    diagonal = F)

```



#### EFA

To confirm this, an EFA was conducted for the two-factor solution
suggested by Parallel analysis. Generalised Least Square (GLS) estimator
was suggested by [Spector & Fleishman (1998)](#references) and [Muthen (1978, 1988)](#references)
but more recent methodological papers suggest WLSM (mean adj weighted least
square - see e.g. mplus) which do not assume a particular distribution.

-   Using WLSM as a factoring method

-   rotation evaluated: Varimax, Promax.

This code performs an loratory factor analysis (EFA) with tetrachoric
correlations using the "fa()" function from the "psych" package. The
analysis is rotated using both the varimax and promax methods, and the
resulting loadings are compared using a heatmap.

The "fa()" function is called twice within a loop, with each iteration
rotating the factor solution using a different method. The loadings from
each iteration are then combined into a single matrix using the
"cbind()" function.

Finally, the resulting loadings matrix is rounded to two decimal places
using the "round()" function and visualized using the "pheatmap()"
function from the "pheatmap" package. The heatmap displays the loadings
for each factor and rotation method, with the colors representing the
strength and direction of the loadings.

The Bartlett correction (correct = "T" in the fa() function) is commonly
used in factor analysis when the assumption of sphericity is violated,
which means that the variables are not uncorrelated and the correlation
matrix is not an identity matrix. In such cases, the eigenvalues of the
correlation matrix tend to overestimate the true number of factors.

The Bartlett correction adjusts for the lack of sphericity by using a
correction factor based on the determinant of the correlation matrix.
The corrected eigenvalues provide a better estimate of the true number
of factors.

This correction is generally appropriate when conducting factor analysis
on correlated variables which is the assumption here.


```{r eval=TRUE, echo=F, error=FALSE, message=FALSE, warning=FALSE}

res.efa<-psych::fa(LV_items_ex[,-1], 
                   nfactors = 2 , 
                   rotate = "none", 
                   fm = "gls", 
                   cor = "poly",
                   correct = "T")

res<-res.efa$loadings[,1:2]


rotat0<- c("varimax", "promax")

# After oblique rotation such as promax or oblimin we have two types of 
# loadings: pattern matrix (regression coefficients, or loadings per se) 
# and structure matrix (correlation coefficients). The regression coefficients 
# of correlated predictors can often lie beyond [-1, 1]. 

for (rmeth in rotat0) {
  
    res.efa<-psych::fa(LV_items_ex[,-1], 
                       nfactors = 2, 
                       rotate = rmeth, 
                       cor = "poly",
                       fm = "gls",
                       correct = "T")
    res<-cbind(res,res.efa$loadings[,1:2])
    ## fa diagram
    #fa.diagram(res.efa, simple = T, main = paste("loratory FA with", rmeth, "rotation"))
    
}

colnames(res)<-c("no-rotation-F1","no-rotation-F2",
                 "varimax-F1", "varimax-F2",
                 "promax-F1","promax-F2")

res<-round(res,2)
res %>% round(2) %>% pheatmap(display_numbers = T, 
                      cluster_rows = F,
                      cluster_cols = F,
                      angle_col = 45,
                      color = colorRampPalette(rev(brewer.pal(n = 7, name = "RdBu")))(100),
                      breaks = seq(-1, 1, by = 0.02),
                      main = "Comparison of loadings between rotations for EFA round 1 50-89yo"
                      )
# Thurstone's (1947) "simplicity criteria":
#
# 1.  Each variable should produce at least one zero loading
#     (-.1<l<.1) on some factor.
# 2.  Each factor should have at least as many zero loadings as there are
#     factors.
# 3.  Each pair of factors should have variables with significant loadings
#     on one (l > .3) and zero loadings on the other.
# 4.  Each pair of factors should have a large proportion of zero loadings
#     on both factors (if there are say four or more factors total).
# 5.  Each pair of factors should have only a few complex variables (l > .3).

#hist(res.efa$scores[,1], main = "Histogram of Factor 1")
#hist(res.efa$scores[,2], main = "Histogram of Factor 2")


```

*Note that in oblique rotations like promax, loadings are regression
coefficients and can exceed the [-1,1] interval.*

**Rotation decision.** Promax rotation was selected as satisfying all 5
Thurstone's criteria for simplicity. The other oblique rotation
(oblimin) was also satisfactory but inferior by the same criteria. This
represent a suggestion that this selection of items comprises
two defined and correlated factors.

#### EFA summary


#### CFA: one vs two-factor fit

For simplicity of interpretation, all CFA models will be identified by
fixing the variance of the latent to 1 and therefore all indicators are
free parameters.

Diagonally weighted least squares (WLSMV) is specifically designed for
ordinal data.

[Kline (2016)](#referecens) suggests to inspect residuals (type = "correlation") to
identify residuals \> \|.10\| (chapter 4). Such residuals indicate a likely
specification error and could inform a decision to respecify a model\*.

##### One factor solution

Like the previous, both models are confirmatory factor analysis (CFA)
models that aim to identify the factor structure of the ADL+IADL data.
The second model, ALL.one.factor.model, proposes a single factor
that loads on all ALL items

```{r eval=TRUE, echo=T, error=FALSE, message=FALSE, warning=FALSE}

######################################################
##### CONFIRMATORY FACTOR ANALYSIS: one.factor.model
#######################################################

ALL.one.factor.model<-'

  Lv1   =~ r2dressa + r2walkra + r2batha + r2eata + r2beda + r2toilta + r2shopa + r2mealsa + r2housewka + r2chaira  + r2clim1a + r2lifta + r2mapa + r2phonea + r2moneya + r2medsa + r2hlthlm + hlmact'
ALL.one.factor.Fit<- cfa(model = ALL.one.factor.model, 
                     data = LV_items_conf,
                     ordered = TRUE, # polychoric correlations for the ordinal variables
                     std.lv = TRUE,
                     auto.fix.first = FALSE,
                     estimator = "WLSM" #  WLSM or alternatively WLSMV
                     )
                                 


r<-residuals(ALL.one.factor.Fit, type = "cor")$cov %>% as.table %>% round(2)
r[abs(r) < 0.1] <- "-"

#r

## Diagnostic tool to explore residual correlations or other potential sources of bias in the model
#m<-modificationindices(ALL.one.factor.Fit)

summary(ALL.one.factor.Fit, fit.measures = T, rsquare=T)

#######################################################
##### END CONFIRMATORY FACTOR ANALYSIS
#######################################################
```



##### Two factor solution

The model below includes two latent variables, "Lv1" and "Lv2", which
are measured by a set of observed variables (indicators). 

.  The first latent variable "Lv1" is measured by 13 binary items: r2eata, r2dressa, r2walkra, r2batha,  r2beda, r2toilta, r2shopa, r2mealsa, r2housewka, r2chaira , r2clim1a, r2lifta, r2hlthlm, and hlmact;

.  The second latent variable "Lv2" is measured by five binary items: r2mapa, r2phonea, r2moneya, r2medsa, and r2eata;

The symbol "=\~" is used to specify the measurement model, indicating
that the latent variables are being measured by the observed variables.
The symbol "\~\~" specifies a correlation between the two latent
variables.

The lavaan package is used to estimate the model using weighted least
squares mean and variance adjusted (WLSMV) estimation. The "ordered =
TRUE" argument indicates that polychoric correlations should be used for
the binary variables. The "auto.fix.first = FALSE" argument specifies
that the first loading for each latent variable should not be fixed to 1
for identification purposes. Finally, the "std.lv = TRUE" argument
specifies that the latent variables should be standardized to have a
variance of 1.

The resulting object "ALL.two.factor.Fit" contains information
about the fit of the CFA model to the data, such as standardized
parameter estimates, fit indices, and residuals.

The first model, ALL.two.factor.model, proposes two factors: a
first factor that loads on the nine items suggested by EFA, and a second
factor that loads on the other four items.

```{r eval=TRUE, echo=T, error=FALSE, message=FALSE, warning=FALSE}

######################################################
##### CONFIRMATORY FACTOR ANALYSIS: Two.factor.model
#######################################################

ALL.two.factor.model<-'

  Lv1   =~ r2dressa + r2walkra + r2batha +  r2beda + r2toilta + r2shopa + r2mealsa + r2housewka + r2chaira  + r2clim1a + r2lifta + r2hlthlm + hlmact
  Lv2   =~ r2mapa + r2phonea + r2moneya + r2medsa + r2eata
  
  Lv1 ~~ Lv2
'
ALL.two.factor.Fit<- cfa(model = ALL.two.factor.model, 
                     data = LV_items_conf,
                     ordered = TRUE, # polychoric correlations for the ordinal variables
                     auto.fix.first = FALSE,
                     std.lv = TRUE,
                     estimator = "WLSM" #  WLSM or alternatively WLSMV
                     )
                                 
#residuals(ALL.two.factor.Fit, type = "cor")

## Diagnostic tool to explore residual correlations or other potential sources of bias in the model
#m<-modificationindices(ALL.two.factor.Fit)
r<-residuals(ALL.two.factor.Fit, type = "cor")$cov %>% as.table %>% round(2)
r[abs(r) < 0.1] <- "-"

#r  # residuals
summary(ALL.two.factor.Fit, fit.measures = T, rsquare=T, standardize = T)

#######################################################
##### END CONFIRMATORY FACTOR ANALYSIS
#######################################################
```



##### One vs two factor fit comparison

lavTestLRT() performs a likelihood ratio test (LRT) comparing two
models, in this case ALL.two.factor.Fit and
ALL.one.factor.Fit. The null hypothesis is that the more complex
model (i.e., ALL.two.factor.Fit) does not fit significantly better
than the simpler model (i.e., ALL.one.factor.Fit). The alternative
hypothesis is that the more complex model fits significantly better than
the simpler model.

```{r eval=TRUE, echo=T, error=FALSE, message=FALSE, warning=FALSE}
a1<-lavTestLRT(ALL.one.factor.Fit,ALL.two.factor.Fit,type="chisq")

a1
```

##### Conclusion: not unidimensional

The output of the lavTestLRT function indicates that the scaled
chi-squared difference test was used with the satorra.bentler.2001
method to compare the fit of the two models.

The Chisq diff indicates a significant difference in fit between the two models. 
The p-value in the Pr(\>Chisq) column < .001, which provides strong
evidence to reject the null hypothesis that the two models fit the data
equally well.

Therefore, we can conclude that the ALL.two.factor model provides a
better fit to the data than the ALL.one.factor model.

This confirms the results of EFA and thus that these items
underlie two separate dimensions.

## GOAL 2: Achieving a one factor solution

### Removing items

Considering that our overall aim is to achieve a unidimensional solution
that can capture the impact of chronic pain on everyday activities we
can eliminate the cognitive focused tasks, and "eating and chopping out food" as
it seems an item that could potentially be sometimes related to cognitive impairment 
rather than being a typical impairment due to pain.


```{r, eval=TRUE,echo=T,warning=FALSE,message=FALSE,error=FALSE}
varlist2<-c("idauniq", 
            "r2dressa", "r2walkra", "r2batha", "r2beda", "r2toilta",
            "r2shopa", "r2mealsa", "r2housewka", "r2hlthlm","hlmact",
            "r2chaira","r2clim1a","r2lifta")


LV_items_ex2<-LV_items_ex[,varlist2]
```

#### Clustering

```{r eval=TRUE, echo=T,}

cor0<-polychoric(LV_items_ex2[,-1], smooth = F, correct = F)

# treats correlation scores as point in a high dimensional space and calculates the distance between two points
polychor.dist<-as.dist(1 - cor0$rho)

# Complete linkage:

# . Calculates the maximum dissimilarity between pairs of clusters.
# . Tends to produce clusters that are compact and well-separated.
# . Can be sensitive to outliers.

hclust(polychor.dist, method="average") %>% plot
```

Notably item "r2chaira" appears to show the greates euclidian distance with the other items.

#### Parallel analysis (Selected items)

This code is performing another parallel analysis to determine the
number of factors that should be retained in a factor analysis of the
LV_items_ex2 dataset (Selected items). The number of factors to retain will be
determined by comparing the actual eigenvalues to the mean and standard
deviation of the simulated eigenvalues generated from the bootstrap
resampled datasets.

```{r, eval=TRUE,echo=T,warning=FALSE,message=FALSE,error=FALSE}

pa<-psych::fa.parallel(LV_items_ex2[,-1],
                   main = "Parallel analysis for EFA round 2",
                   fa ="fa", 
                   fm = "gls", 
                   sim = "F", 
                   error.bars = T, 
                   cor = "poly", 
                   correct = T)


```

Removing the cognitive items and r2eata, parallel analysis that a one factor solution is
acceptable


```{r, eval=TRUE,echo=T,warning=FALSE,message=FALSE,error=FALSE}

cor_vss2<-polychoric(LV_items_ex2[,-1], smooth = T, correct = T)


VSS(cor_vss2$rho, 
    fm="mle", 
    rotate = "oblimin",
    n=4,
    n.obs = nrow(LV_items_ex2[,-1]), 
    diagonal = F)
```

#### EFA summary (Selected items)

This code conduct a factor analysis using the fa() function:

```{r eval=TRUE, echo=T, error=FALSE, message=FALSE, warning=FALSE}

res.efa2<-psych::fa(LV_items_ex2[,-1], 
                   nfactors = 1, 
                  # rotate = "promax", 
                   fm = "gls", 
                   cor = "poly",
                   correct = "T")
res.efa2 %>% loadings()
```



##### Balancing the measure


Looking at the dendogram of the hierarchical realtionship and considering the loadings from the EFA 
using promax rotation, we will attempt to simplify a selection of items that would strike a measure 
of impact that could be more balanced and not too focused on pure physical disability. 

we would exclude 

*  "r2chaira"
*  "r2batha","r2toilta","r2dressa"

to have nine items:

r2shopa + r2housewka 
r2clim1a + r2lifta + r2walkra 
r2mealsa + r2beda
r2hlthlm + hlmact 





#### CFA: Selected items

The model assumes that all the observed variables are indicators of a
single underlying construct, which is represented by the latent variable
Lv1.



```{r eval=TRUE, echo=T, error=FALSE, message=FALSE, warning=FALSE}

######################################################
##### CONFIRMATORY FACTOR ANALYSIS: One.factor.model
#######################################################

One.factor.model<-'

  Lv1   =~  r2shopa + r2housewka + r2walkra + r2mealsa +
            r2clim1a + r2lifta +
            r2beda +
            r2hlthlm #+ hlmact 

'
One.factor.Fit<- cfa(model = One.factor.model, 
                     data = LV_items_conf,
                     ordered = TRUE, # polychoric correlations for the ordinal variables
                     std.lv = TRUE,
                     auto.fix.first = FALSE,
                     estimator = "WLSM" #  WLSM or alternatively WLSMV
                     )
                                 

## Diagnostic tool to explore residual correlations or other potential sources of bias in the model
#m<-modificationindices(One.factor.Fit)

summary(One.factor.Fit, fit.measures = T)
#coef(One.factor.Fit) %>% as.matrix %>% round(3)
#######################################################
##### END CONFIRMATORY FACTOR ANALYSIS
#######################################################
```

In the followin table we see the factor loadings and thresholds obtained
from the one-factor confirmatory factor analysis model fitted to the
LV_items_conf dataset. The factor loadings represent the strength of the
relationship between each item and the latent factor
(ability level) being measured, while the thresholds represent the point
at which each item transitions from one response category to the next in
the ordered categorical measurement scale used in the model.

```{r}
thresholds <- lavInspect(One.factor.Fit, "thresholds") %>% as.matrix()
thresholds[order(thresholds,decreasing = T),] %>% as.matrix()

```


Residuals correlations
```{r}
r<-residuals(One.factor.Fit, type = "cor")$cov %>% as.table %>% round(2)
r[abs(r) < 0.1] <- "-"

r
```
Residual correlation below .1 are omitted (-).

# CONCLUSION

This is the suggested set of items to be studied with IRT

```{r}

list<-c(varlist,"inw2","inw3","r2painfr","r2painlv","r2agey","ragender","r2chrpain")




LV_items.irt<-H_elsa_w2_[list]



write.csv(LV_items.irt,"C:/Users/d.vitali/Desktop/Github/CRIISP-WP5/data/clean/cfa_w2_all")
```

# References

* Banks, J., Marmot, M., Blundell, R., Lessof, C., & Nazroo, J. (2003). Health, wealth and lifestyles of the older population in England: the 2002 English Longitudinal Study of Ageing. In. Institute for Fiscal Studies. 
* Caballero, F. F., Soulis, G., Engchuan, W., Sánchez-Niubó, A., Arndt, H., Ayuso-Mateos, J. L., ... & Panagiotakos, D. B. (2017). Advanced analytical methodologies for measuring healthy ageing and its determinants, using factor analysis and machine learning techniques: the ATHLOS project. Scientific reports, 7(1), 43955.
* Fieo, R. A., Austin, E. J., Starr, J. M., & Deary, I. J. (2011). Calibrating ADL-IADL scales to improve measurement accuracy and to extend the disability construct into the preclinical range: a systematic review. BMC geriatrics, 11, 1-15.
* Katz, S. (1963). The index of ADL: a standardized measure of biological and psychosocial function. J Am Med Assoc, 185, 914-919.
* Kempen, G. I. J. M., Myers, A. M., & Powell, L. E. (1995). Hierarchical structure in ADL and IADL: analytical assumptions and applications for clinicians and researchers. Journal of clinical epidemiology, 48(11), 1299-1305.
* Kline, R. B. (2023). Principles and practice of structural equation modeling. Guilford publications.
* Kovar, M. G., & Powell Lawton, M. (1994). Functional disability: activities and instrumental activities of daily living. Annual review of gerontology and geriatrics, 14, 57-57.
* Lawton MP, Brody EM. Assessment of older people: Self-maintaining and instrumental activities of daily living. Gerontologist. 1969;9(3):179–86.
* Mindell, J., Biddulph, J. P., Hirani, V., Stamatakis, E., Craig, R., Nunn, S., & Shelton, N. (2012). Cohort profile: the health survey for England. International journal of epidemiology, 41(6), 1585-1593.
* Muthén, B. (1978). Contributions to factor analysis of dichotomous variables. Psychometrika, 43(4), 551-560.
* Muthén, B., & Hofacker, C. (1988). Testing the assumptions underlying tetrachoric correlations. Psychometrika, 53, 563-577.
* Newsom, J. (2023) Structural equation modelling
* Revelle, W., & Rocklin, T. (1979). Very simple structure: An alternative procedure for estimating the optimal number of interpretable factors. Multivariate behavioral research, 14(4), 403-414.
* Revelle, W. (2024). psych: Procedures for Psychological, Psychometric, and Personality Research. Northwestern University, Evanston,
  Illinois. R package version 2.4.3, <https://CRAN.R-project.org/package=psych>.
* Spector, W. D. and Fleishman, J. A. (1998). Combining activities of daily living with instrumental activities of daily living to measure functional disability. J. Gerontology: Social Sciences 
* Thomas, V. S., Rockwood, K., & McDowell, I. (1998). Multidimensionality in instrumental and basic activities of daily living. Journal of clinical epidemiology, 51(4), 315-321.
* Thurstone, L. L. (1947). Multiple-factor analysis; A development and expansion of The vectors of the mind. Chicago, IL: University of Chicago Press.
